# -*- coding: utf-8 -*-
"""ezr&d.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zHy0_0obub2k87kO2gf6GEG7k5IWXYlw
"""

# imports
import sys
import os
import torch
import numpy as np
from tqdm import tqdm
import torch.nn as nn
import pandas as pd
import torch.nn.functional as F
from torch.autograd import Variable
import torch.utils.data as Data
import matplotlib.pyplot as plt
from torch.optim import Adam, SGD
from sklearn.metrics import confusion_matrix
from keras.layers import Dense, Convolution1D, MaxPool1D, Flatten, Dropout
from keras.layers import Input, LSTM
from keras.models import Model
from tensorflow import keras
from tensorflow.keras import layers
# from keras.layers.normalization import BatchNormalization
from keras.layers import BatchNormalization
import keras
from matplotlib.pyplot import figure
from keras.callbacks import EarlyStopping, ModelCheckpoint

#read the data
raw_data =pd.read_csv('Epileptic Seizure Detection.csv')
raw_data.shape

raw_data

data = raw_data.values
data = data[1:11501, 1:180]
#data[:, 178] = data[:, 178].astype(int)
data.shape

type(data)

# Classifying data categories
# The shape of a data frame tells you how many rows and columns it has.

D = data
df_1 = D[D[:, 178]==1]
df_2 = D[D[:, 178]==2]
df_3 = D[D[:, 178]==3]
df_4 = D[D[:, 178]==4]
df_5 = D[D[:, 178]==5]

print(df_1.shape)
print(df_2.shape)
print(df_3.shape)
print(df_4.shape)
print(df_5.shape)

df_1 = df_1.astype(int)
df_2 = df_2.astype(int)
df_3 = df_3.astype(int)
df_4 = df_4.astype(int)
df_5 = df_5.astype(int)

# some operations on the df_3 data frame and then concatenating df_1 and the modified df_3 into a new array D1
df_3[:, 178] = df_3[:, 178] - 3
D1 = np.concatenate([df_1, df_3])

# Creating training (80), validation (10) and test (10) data from category 1 and 3
number_of_rows = D1.shape[0]

random_indices = np.random.choice(number_of_rows, size=int(number_of_rows*0.8), replace=False)

label_train = D1[random_indices, -1]
data_train = D1[random_indices, :-1]

D1_rest = np.delete(D1, random_indices, 0)

number_of_rows = D1_rest.shape[0]
random_indices = np.random.choice(number_of_rows, size=int(number_of_rows*0.5), replace=False)

label_val = D1_rest[random_indices, -1]
data_val = D1_rest[random_indices, :-1]

D1_rest_rest = np.delete(D1_rest, random_indices, 0)
label_test = D1_rest_rest[:, -1]
data_test = D1_rest_rest[:, :-1]

data_train = np.expand_dims(data_train, axis=2)
data_val = np.expand_dims(data_val, axis=2)
data_test = np.expand_dims(data_test, axis=2)

print(label_train.shape, data_train.shape)
print(label_val.shape, data_val.shape)
print(label_test.shape, data_test.shape)

from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, accuracy_score
import numpy as np

def evaluate_model(X_test, y_test, model, threshold=0.5):
    prediction_proba = model.predict(X_test)

    # Convert prediction probabilities to binary predictions
    prediction = (prediction_proba > threshold).astype(int)

    accuracy = accuracy_score(y_test, prediction)
    precision = precision_score(y_test, prediction)
    recall = recall_score(y_test, prediction)
    f1 = f1_score(y_test, prediction)

    cnf_matrix = confusion_matrix(y_test, prediction)

    true_negatives, false_positives, false_negatives, true_positives = cnf_matrix.ravel()
    sensitivity = true_positives / (true_positives + false_negatives)
    specificity = true_negatives / (true_negatives + false_positives)

    print("Accuracy: %.2f%%" % (accuracy * 100))
    print("Precision: %.2f%%" % (precision * 100))
    print("Recall: %.2f%%" % (recall * 100))
    print("F1-score: %.2f%%" % (f1 * 100))
    print("Sensitivity (True Positive Rate): %.2f%%" % (sensitivity * 100))
    print("Specificity (True Negative Rate): %.2f%%" % (specificity * 100))

    return accuracy

import keras
from keras.models import Sequential
from keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout
from keras.regularizers import l2

# Define 2D-CNN network
def network_CNN(X_train, y_train):
    im_shape = (X_train.shape[1], 1)
    model = Sequential()

    # Convolutional layers
    model.add(Conv1D(filters=8, kernel_size=6, input_shape=im_shape))
    model.add(MaxPooling1D(pool_size=2))
    model.add(Conv1D(filters=16, kernel_size=3))
    model.add(MaxPooling1D(pool_size=2))

    # Flatten the output
    model.add(Flatten())

    # Fully connected layers with increased regularization and dropout
    model.add(Dense(32, kernel_regularizer=l2(0.5), activation='relu'))
    model.add(Dropout(0.8))
    model.add(Dense(16, kernel_regularizer=l2(0.5), activation='relu'))

    # Output layer
    model.add(Dense(1, activation='sigmoid'))

    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

    return model

# Define 2D-CNN model to be trained on epileptic vs healthy data
model1 = network_CNN(data_train,label_train)
print(model1.summary())

# Train the model
history1 = model1.fit(data_train, label_train, epochs=5, batch_size=32, validation_data=(data_val, label_val))

# After evaluating the model and making predictions on the test data
a1 = evaluate_model(data_test, label_test, model1)
y_pred = model1.predict(data_test)

import matplotlib.pyplot as plt

# Extract training history
training_accuracy = history1.history['accuracy']
validation_accuracy = history1.history['val_accuracy']
training_loss = history1.history['loss']
validation_loss = history1.history['val_loss']

# Plot training and validation accuracy
plt.figure(figsize=(12, 4))
plt.subplot(1, 2, 1)
plt.plot(training_accuracy, label='Training Accuracy')
plt.plot(validation_accuracy, label='Validation Accuracy')
plt.title('Training and Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()

# Plot training and validation loss
plt.subplot(1, 2, 2)
plt.plot(training_loss, label='Training Loss')
plt.plot(validation_loss, label='Validation Loss')
plt.title('Training and Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

plt.tight_layout()
plt.show()

from keras.layers import Bidirectional, LSTM, Dense, Input, Dropout, BatchNormalization
from keras.models import Model

def network_BiLSTM(X_train, y_train):
    im_shape = (X_train.shape[1], 1)
    inputs_lstm = Input(shape=(im_shape), name='inputs_lstm')

    lstm = Bidirectional(LSTM(units=128, return_sequences=False))(inputs_lstm)
    dropout = Dropout(0.3)(lstm)
    batch_normalization = BatchNormalization()(dropout)
    dense_1 = Dense(units=64, activation='relu')(batch_normalization)
    dropout_2 = Dropout(0.3)(dense_1)
    batch_normalization_1 = BatchNormalization()(dropout_2)
    main_output = Dense(units=1, activation='sigmoid')(batch_normalization_1)  # Use 'sigmoid' for binary classification

    model = Model(inputs=inputs_lstm, outputs=main_output)
    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])  # Use 'binary_crossentropy' for binary classification

    return model

# Create and compile the BiLSTM model
model2 = network_BiLSTM(data_train, label_train)

# Print model summary
print(model2.summary())

# Define a ModelCheckpoint callback to save the best model
save_path = '/tmp/checkpoint_2'
model_checkpoint_callback = keras.callbacks.ModelCheckpoint(
    filepath=save_path,
    save_weights_only=True,
    monitor='val_accuracy',
    mode='max',
    save_best_only=True
)

# Train the BiLSTM model
history2 = model2.fit(data_train, label_train, epochs=5, batch_size=32, validation_data=(data_val, label_val), callbacks=[model_checkpoint_callback])

# Evaluate the model on the test data
a2 = evaluate_model(data_test, label_test, model2)

# Extract training history
training_accuracy = history2.history['accuracy']
validation_accuracy = history2.history['val_accuracy']
training_loss = history2.history['loss']
validation_loss = history2.history['val_loss']

# Plot training and validation accuracy
plt.figure(figsize=(12, 4))
plt.subplot(1, 2, 1)
plt.plot(training_accuracy, label='Training Accuracy')
plt.plot(validation_accuracy, label='Validation Accuracy')
plt.title('Training and Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()

# Plot training and validation loss
plt.subplot(1, 2, 2)
plt.plot(training_loss, label='Training Loss')
plt.plot(validation_loss, label='Validation Loss')
plt.title('Training and Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt
import numpy as np

def compare_accuracies(accuracy1, accuracy2, algorithm1_name, algorithm2_name):
    # Data
    accuracies = [accuracy1, accuracy2]
    algorithm_names = [algorithm1_name, algorithm2_name]

    # Create a bar chart
    plt.figure(figsize=(6, 6))
    plt.bar(algorithm_names, accuracies, color=['blue', 'green'])
    plt.ylabel('Accuracy (%)')
    plt.title('Algorithm Comparison - Accuracy')

    # Display the accuracy values on top of the bars
    for i, v in enumerate(accuracies):
      plt.text(i, v, f'{v:.2f}', ha='center', va='bottom')
    # Show the bar chart
    plt.show()

# Example usage:
accuracy_algorithm1 = 93.04 # Replace with the actual accuracy of algorithm 1
accuracy_algorithm2 = 98.48  # Replace with the actual accuracy of algorithm 2
algorithm1_name = '2D-CNN'
algorithm2_name = 'Bi-LSTM'

compare_accuracies(accuracy_algorithm1, accuracy_algorithm2, algorithm1_name, algorithm2_name)

import matplotlib.pyplot as plt
import numpy as np

def compare_accuracies(accuracy1, accuracy2, algorithm1_name, algorithm2_name):
    # Data
    accuracies = [accuracy1, accuracy2]
    algorithm_names = [algorithm1_name, algorithm2_name]

    # Create a bar chart
    plt.figure(figsize=(6, 6))
    plt.bar(algorithm_names, accuracies, color=['skyblue', 'brown'])
    plt.ylabel('Sensitivity (%)')
    plt.title('Algorithm Comparison - Sensitivity')

    # Display the accuracy values on top of the bars
    for i, v in enumerate(accuracies):
      plt.text(i, v, f'{v:.2f}', ha='center', va='bottom')
    # Show the bar chart
    plt.show()

# Example usage:
accuracy_algorithm1 = 90.79 # Replace with the actual accuracy of algorithm 1
accuracy_algorithm2 = 97.81  # Replace with the actual accuracy of algorithm 2
algorithm1_name = '2D-CNN'
algorithm2_name = 'Bi-LSTM'

compare_accuracies(accuracy_algorithm1, accuracy_algorithm2, algorithm1_name, algorithm2_name)

import matplotlib.pyplot as plt
import numpy as np

def compare_accuracies(accuracy1, accuracy2, algorithm1_name, algorithm2_name):
    # Data
    accuracies = [accuracy1, accuracy2]
    algorithm_names = [algorithm1_name, algorithm2_name]

    # Create a bar chart
    plt.figure(figsize=(6, 6))
    plt.bar(algorithm_names, accuracies, color=['orange', 'grey'])
    plt.ylabel('Specificity (%)')
    plt.title('Algorithm Comparison - Specificity')

    # Display the accuracy values on top of the bars
    for i, v in enumerate(accuracies):
      plt.text(i, v, f'{v:.2f}', ha='center', va='bottom')
    # Show the bar chart
    plt.show()

# Example usage:
accuracy_algorithm1 = 95.48 # Replace with the actual accuracy of algorithm 1
accuracy_algorithm2 = 99.14  # Replace with the actual accuracy of algorithm 2
algorithm1_name = '2D-CNN'
algorithm2_name = 'Bi-LSTM'

compare_accuracies(accuracy_algorithm1, accuracy_algorithm2, algorithm1_name, algorithm2_name)